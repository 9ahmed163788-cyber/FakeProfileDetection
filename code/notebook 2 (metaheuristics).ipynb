{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":11201913,"sourceType":"datasetVersion","datasetId":6994036},{"sourceId":11202266,"sourceType":"datasetVersion","datasetId":6994269}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## 1. Data Preparation","metadata":{}},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:42:19.458060Z","iopub.execute_input":"2025-05-30T09:42:19.458567Z","iopub.status.idle":"2025-05-30T09:42:19.465714Z","shell.execute_reply.started":"2025-05-30T09:42:19.458541Z","shell.execute_reply":"2025-05-30T09:42:19.465068Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_path = \"/kaggle/input/dataset-x-fake-profile-zip/dataset_x_fake_profile/\"\nprint(f\"ğŸ“‚ Dataset path: {dataset_path}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:42:19.467337Z","iopub.execute_input":"2025-05-30T09:42:19.467806Z","iopub.status.idle":"2025-05-30T09:42:19.489103Z","shell.execute_reply.started":"2025-05-30T09:42:19.467785Z","shell.execute_reply":"2025-05-30T09:42:19.488229Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\n\ndataset_path = \"/kaggle/input/dataset-x-fake-profile-zip/dataset_x_fake_profile/\"\nbase_len = len(dataset_path)\ntotal_images = 0\n\nfor root, dirs, files in os.walk(dataset_path):\n    num_images = len([f for f in files if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n    if num_images > 0:\n        short_path = root[base_len:]\n        total_images += num_images\n        print(f\"ğŸ“ Folder: {short_path} - ğŸ–¼ï¸ Number of images: {num_images}\")\n\nprint(\"\\nğŸ“Š Total number of images in the dataset:\", total_images)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:42:19.490067Z","iopub.execute_input":"2025-05-30T09:42:19.490294Z","iopub.status.idle":"2025-05-30T09:42:44.699817Z","shell.execute_reply.started":"2025-05-30T09:42:19.490275Z","shell.execute_reply":"2025-05-30T09:42:44.698973Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom tqdm import tqdm\nfrom multiprocessing import Pool, cpu_count\n\ninput_root = \"/kaggle/input/dataset-x-fake-profile-zip/dataset_x_fake_profile\"\noutput_root = \"/kaggle/working/dataset_x_normalized\"\n\ndef min_max_normalization(img):\n    img = img.astype(np.float32)\n    min_val = np.min(img, axis=(0, 1), keepdims=True)\n    max_val = np.max(img, axis=(0, 1), keepdims=True)\n    range_val = max_val - min_val\n    range_val[range_val == 0] = 1  # Avoid division by zero\n    normalized_img = (img - min_val) / range_val * 255\n    return np.clip(normalized_img, 0, 255).astype(np.uint8)\n\ndef process_image(args):\n    input_path, output_path, normalization_func = args\n    img = cv2.imread(input_path)\n    if img is None:\n        return False  # Failed to load image\n\n    norm_img = normalization_func(img)\n    if len(norm_img.shape) == 2:\n        norm_img = cv2.cvtColor(norm_img, cv2.COLOR_GRAY2BGR)\n\n    cv2.imwrite(output_path, norm_img)\n    return True\n\ndef process_and_save_images(normalization_func, technique_name):\n    for split in [\"train\", \"test\", \"validation\"]:\n        for category in [\"verified\", \"real\", \"cyborg\", \"bot\"]:\n            input_dir = os.path.join(input_root, split, category)\n            output_dir = os.path.join(output_root, technique_name, split, category)\n            os.makedirs(output_dir, exist_ok=True)\n\n            images = os.listdir(input_dir)\n            total_images = len(images)\n\n            args_list = []\n            for img_name in images:\n                input_path = os.path.join(input_dir, img_name)\n                output_path = os.path.join(output_dir, img_name)\n                args_list.append((input_path, output_path, normalization_func))\n\n            with Pool(processes=cpu_count()) as pool:\n                results = list(tqdm(pool.imap_unordered(process_image, args_list), total=total_images,\n                                    desc=f\"ğŸ”„ {split}/{category} ({technique_name})\", unit=\"image\"))\n\n    print(f\"âœ… {technique_name} normalization applied and images saved successfully!\")\n\nprocess_and_save_images(min_max_normalization, \"min_max\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:42:44.702028Z","iopub.execute_input":"2025-05-30T09:42:44.702263Z","iopub.status.idle":"2025-05-30T09:56:16.948471Z","shell.execute_reply.started":"2025-05-30T09:42:44.702244Z","shell.execute_reply":"2025-05-30T09:56:16.947410Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\n\nbase_data_dir = \"/kaggle/working/dataset_x_normalized/min_max\" \n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nsplits = [\"train\", \"validation\", \"test\"]\ncategories = [\"verified\", \"real\", \"cyborg\", \"bot\"]\n\nfor split in splits:\n    for category in categories:\n        path = os.path.join(base_data_dir, split, category)\n        if not os.path.exists(path):\n            print(f\"âš ï¸ Warning: Folder {path} does not exist!\")\n        else:\n            print(f\"ğŸ“‚ Path exists: {path}\")\n\nprint(f\"\\nğŸ–¥ï¸ Device in use: {device}\")\n\nprint(f\"ğŸ“‚ Test data path: {os.path.join(base_data_dir, 'test')}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:56:16.949860Z","iopub.execute_input":"2025-05-30T09:56:16.950145Z","iopub.status.idle":"2025-05-30T09:56:27.555401Z","shell.execute_reply.started":"2025-05-30T09:56:16.950118Z","shell.execute_reply":"2025-05-30T09:56:27.554518Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 2. Image Transformations\n","metadata":{}},{"cell_type":"code","source":"from torchvision import transforms\n\ntransform_train = transforms.Compose([\n    transforms.Resize(256),\n    transforms.RandomCrop(224),\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.RandomRotation(15),\n    transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), shear=10),\n    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n    transforms.RandomGrayscale(p=0.1),\n    transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n    transforms.ToTensor(),\n    transforms.RandomErasing(p=0.4),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\ntransform_val_test = transforms.Compose([\n    transforms.Resize(256),\n    transforms.CenterCrop(224),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n])\n\nprint(\"âœ… Image transformations set successfully!\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:56:27.556368Z","iopub.execute_input":"2025-05-30T09:56:27.556865Z","iopub.status.idle":"2025-05-30T09:56:32.337301Z","shell.execute_reply.started":"2025-05-30T09:56:27.556834Z","shell.execute_reply":"2025-05-30T09:56:32.336629Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3. Data Loading","metadata":{}},{"cell_type":"code","source":"from torchvision.datasets import ImageFolder\nfrom torch.utils.data import DataLoader\nimport torch\nimport os\n\nbase_data_dir = \"/kaggle/working/dataset_x_normalized/min_max\"\n\ntrain_path = os.path.join(base_data_dir, \"train\")\nvalidation_path = os.path.join(base_data_dir, \"validation\")\ntest_path = os.path.join(base_data_dir, \"test\")\n\nassert os.path.exists(train_path), f\"âŒ Train path does not exist: {train_path}\"\nassert os.path.exists(validation_path), f\"âŒ Validation path does not exist: {validation_path}\"\nassert os.path.exists(test_path), f\"âŒ Test path does not exist: {test_path}\"\n\ntrain_dataset = ImageFolder(root=train_path, transform=transform_train)\nvalidation_dataset = ImageFolder(root=validation_path, transform=transform_val_test)\ntest_dataset = ImageFolder(root=test_path, transform=transform_val_test)\n\nprint(\"âœ… Data loaded successfully!\")\nprint(f\"ğŸ”¹ Number of training images: {len(train_dataset)}\")\nprint(f\"ğŸ”¹ Number of validation images: {len(validation_dataset)}\")\nprint(f\"ğŸ”¹ Number of test images: {len(test_dataset)}\")\n\nuse_gpu = torch.cuda.is_available()\nnum_workers = min(8, os.cpu_count() // 2) if use_gpu else min(4, os.cpu_count() // 4)\npin_memory = use_gpu\nprefetch_factor = 4 if num_workers > 0 else None\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=32, shuffle=True,\n    num_workers=num_workers, pin_memory=pin_memory,\n    persistent_workers=(use_gpu and num_workers > 0), prefetch_factor=prefetch_factor,\n    drop_last=False\n)\n\nvalidation_loader = DataLoader(\n    validation_dataset, batch_size=32, shuffle=False,\n    num_workers=num_workers, pin_memory=pin_memory,\n    persistent_workers=(use_gpu and num_workers > 0), prefetch_factor=prefetch_factor\n)\n\ntest_loader = DataLoader(\n    test_dataset, batch_size=32, shuffle=False,\n    num_workers=num_workers, pin_memory=pin_memory,\n    persistent_workers=(use_gpu and num_workers > 0), prefetch_factor=prefetch_factor\n)\n\nprint(\"ğŸš€ DataLoaders are ready to use!\")\n","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2025-05-30T09:56:32.338236Z","iopub.execute_input":"2025-05-30T09:56:32.339186Z","iopub.status.idle":"2025-05-30T09:56:32.390640Z","shell.execute_reply.started":"2025-05-30T09:56:32.339164Z","shell.execute_reply":"2025-05-30T09:56:32.389800Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4. Model Architecture","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torchvision import models\n\nweights_path = \"/kaggle/input/resnet50-11ad3fa6-pth/resnet50-11ad3fa6.pth\"\nmodel = models.resnet50()\nmodel.load_state_dict(torch.load(weights_path, map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n\nfor param in model.parameters():\n    param.requires_grad = False\n\nfor layer in [model.layer2, model.layer3, model.layer4]:\n    for param in layer.parameters():\n        param.requires_grad = True\n\nnum_features = model.fc.in_features\nmodel.fc = nn.Sequential(\n    nn.Flatten(),\n    nn.Linear(num_features, 1024),\n    nn.BatchNorm1d(1024),\n    nn.LeakyReLU(0.1, inplace=True),\n    nn.Dropout(0.25),\n    nn.Linear(1024, 512),\n    nn.BatchNorm1d(512),\n    nn.LeakyReLU(0.1, inplace=True),\n    nn.Dropout(0.15),\n    nn.Linear(512, 4)\n)\n\nfor param in model.fc.parameters():\n    param.requires_grad = True\n\ndevice = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\nmodel = model.to(device)\n\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"âœ… Number of trainable parameters: {trainable_params:,}\")\nprint(model.fc)\nprint(\"âœ… Model is successfully prepared for training ğŸ¯\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:56:32.391632Z","iopub.execute_input":"2025-05-30T09:56:32.391967Z","iopub.status.idle":"2025-05-30T09:56:34.502220Z","shell.execute_reply.started":"2025-05-30T09:56:32.391937Z","shell.execute_reply":"2025-05-30T09:56:34.501504Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5. Optimizer & Scheduler\n","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch.optim as optim\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()),\n                        lr=3e-4, weight_decay=1e-4, eps=1e-8)\n\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=0.003,\n    steps_per_epoch=len(train_loader),\n    epochs=10,\n    pct_start=0.2,\n    anneal_strategy='cos',\n    div_factor=25,\n    final_div_factor=1000\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:56:34.504652Z","iopub.execute_input":"2025-05-30T09:56:34.504951Z","iopub.status.idle":"2025-05-30T09:56:34.511228Z","shell.execute_reply.started":"2025-05-30T09:56:34.504931Z","shell.execute_reply":"2025-05-30T09:56:34.510494Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7. Final Training\n","metadata":{}},{"cell_type":"markdown","source":"### 7.1 Main Training Function","metadata":{}},{"cell_type":"code","source":"from tqdm import tqdm\nimport gc\nimport torch\n\ndef train_one_epoch(model, train_loader, optimizer, criterion, device, epoch, start_batch=0):\n    model.train()\n    running_loss, correct, total = 0.0, 0, 0\n    total_batches = len(train_loader)\n\n    scaler = torch.cuda.amp.GradScaler() if torch.cuda.is_available() else None\n    progress_bar = tqdm(enumerate(train_loader), total=total_batches, desc=f\"ğŸš€ Epoch {epoch}\", unit=\"batch\", leave=False)\n\n    for batch_idx, (images, labels) in progress_bar:\n        if batch_idx < start_batch:\n            continue\n\n        images, labels = images.to(device), labels.to(device)\n        optimizer.zero_grad()\n\n        with torch.cuda.amp.autocast() if scaler else torch.enable_grad():\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n        if scaler:\n            scaler.scale(loss).backward()\n            scaler.unscale_(optimizer)\n        else:\n            loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        if scaler:\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            optimizer.step()\n\n        running_loss += loss.item() * images.size(0)\n        correct += outputs.argmax(dim=1).eq(labels).sum().item()\n        total += labels.size(0)\n\n        avg_loss = running_loss / total if total > 0 else 0\n        accuracy = 100.0 * correct / total if total > 0 else 0\n        progress_bar.set_postfix(loss=avg_loss, acc=f\"{accuracy:.2f}%\", batch=batch_idx + 1)\n\n        # ğŸ§¹ Ø¥Ø¯Ø§Ø±Ø© Ø§Ù„Ø°Ø§ÙƒØ±Ø©\n        if torch.cuda.is_available() and batch_idx % 10 == 0:\n            del images, labels, outputs, loss\n            torch.cuda.empty_cache()\n            gc.collect()\n\n    avg_loss = running_loss / total if total > 0 else 0\n    accuracy = 100.0 * correct / total if total > 0 else 0\n\n    print(f\"âœ… Training completed for Epoch {epoch} - Loss: {avg_loss:.4f}, Accuracy: {accuracy:.2f}%\")\n\n    return avg_loss, accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:56:34.512102Z","iopub.execute_input":"2025-05-30T09:56:34.512417Z","iopub.status.idle":"2025-05-30T09:56:34.537402Z","shell.execute_reply.started":"2025-05-30T09:56:34.512391Z","shell.execute_reply":"2025-05-30T09:56:34.536547Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 8. Evaluation & Logging","metadata":{}},{"cell_type":"code","source":"import os\nimport gc\nfrom datetime import datetime\nimport torch\n\ndef train_model(model, train_loader, validation_loader, criterion, optimizer, scheduler, num_epochs, base_save_path, start_epoch=1, early_stopping_patience=5):\n    patience_counter = 0\n    best_validation_loss = float(\"inf\")\n\n    results_dir = os.path.join(base_save_path)\n    os.makedirs(results_dir, exist_ok=True)\n\n    results_file = os.path.join(results_dir, \"training_results.txt\")\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    model.to(device)\n\n    with open(results_file, \"a\", encoding=\"utf-8\") as f:\n        if os.stat(results_file).st_size == 0:\n            f.write(\"ğŸ“Œ Training Log\\n\" + \"=\" * 50 + \"\\n\")\n\n    for epoch in range(start_epoch, num_epochs + 1):\n        start_time = datetime.now()\n        print(f\"\\nğŸ”¥ Training Epoch {epoch}/{num_epochs} ...\")\n\n        train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, criterion, device, epoch)\n\n        with torch.no_grad():\n            validation_loss, validation_acc = validate_model(model, validation_loader, criterion, device)\n\n        if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n            scheduler.step(validation_loss)\n        else:\n            scheduler.step()\n\n        elapsed_time = (datetime.now() - start_time).total_seconds()\n        hours, remainder = divmod(elapsed_time, 3600)\n        minutes, seconds = divmod(remainder, 60)\n        elapsed_str = f\"{int(hours)}h {int(minutes)}m {seconds:.2f}s\"\n\n        best_model_path = os.path.join(base_save_path, \"best_model.pth\")\n        if validation_loss < best_validation_loss:\n            best_validation_loss = validation_loss\n            patience_counter = 0\n            torch.save(model.state_dict(), best_model_path)\n            print(f\"ğŸ† Best model saved at: {best_model_path}\")\n        else:\n            patience_counter += 1\n            print(f\"âš ï¸ No improvement for {patience_counter} consecutive epochs (Patience: {early_stopping_patience})\")\n\n            if patience_counter >= early_stopping_patience:\n                print(\"ğŸ›‘ Early stopping triggered due to no improvement.\")\n                break\n\n        with open(results_file, \"a\", encoding=\"utf-8\") as f:\n            f.write(f\"\\nğŸ—“ {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Epoch {epoch}/{num_epochs}\\n\")\n            f.write(\"=\" * 50 + \"\\n\")\n            f.write(f\"â³ Elapsed Time: {elapsed_str}\\n\")\n            f.write(f\"ğŸ¯ Training: Loss = {train_loss:.4f}, Accuracy = {train_acc:.2f}%\\n\")\n            f.write(f\"âœ… Validation: Loss = {validation_loss:.4f}, Accuracy = {validation_acc:.2f}%\\n\")\n            f.write(\"-\" * 50 + \"\\n\")\n\n        gc.collect()\n        torch.cuda.empty_cache()\n\n    print(f\"âœ… Training completed successfully! Results saved at: {results_file}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:56:34.538456Z","iopub.execute_input":"2025-05-30T09:56:34.538805Z","iopub.status.idle":"2025-05-30T09:56:34.568253Z","shell.execute_reply.started":"2025-05-30T09:56:34.538776Z","shell.execute_reply":"2025-05-30T09:56:34.567221Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Number of samples in train_dataset: {len(train_dataset)}\")\nprint(f\"Number of batches in train_loader: {len(train_loader)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:56:34.569352Z","iopub.execute_input":"2025-05-30T09:56:34.569710Z","iopub.status.idle":"2025-05-30T09:56:34.596718Z","shell.execute_reply.started":"2025-05-30T09:56:34.569681Z","shell.execute_reply":"2025-05-30T09:56:34.595925Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"Batch size used in train_loader: {train_loader.batch_size}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:56:34.597542Z","iopub.execute_input":"2025-05-30T09:56:34.597822Z","iopub.status.idle":"2025-05-30T09:56:34.616068Z","shell.execute_reply.started":"2025-05-30T09:56:34.597801Z","shell.execute_reply":"2025-05-30T09:56:34.615208Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 8.1 Validation with TTA","metadata":{}},{"cell_type":"code","source":"import torch\nfrom tqdm import tqdm\n\ndef tta_predictions(model, images):\n    augmentations = [\n        images,\n        torch.flip(images, [3]),\n        torch.rot90(images, 1, [2, 3]),\n        torch.flip(images, [2]),\n    ]\n\n    with torch.no_grad(), torch.cuda.amp.autocast():\n        outputs = torch.cat([model(aug) for aug in augmentations]).view(len(augmentations), -1, 4).mean(dim=0)\n    return outputs\n\ndef validate_model(model, validation_loader, criterion, device):\n    model.eval()\n    running_loss, correct, total = 0.0, 0, 0\n\n    progress_bar = tqdm(validation_loader, desc=\"ğŸ§ Validation\", unit=\"batch\", leave=False)\n\n    with torch.no_grad():\n        for images, labels in progress_bar:\n            images, labels = images.to(device), labels.to(device)\n\n            outputs = tta_predictions(model, images)\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item() * images.size(0)\n            correct += outputs.argmax(dim=1).eq(labels).sum().item()\n            total += labels.size(0)\n\n            avg_loss = running_loss / total if total > 0 else 0\n            accuracy = 100.0 * correct / total if total > 0 else 0\n            progress_bar.set_postfix(loss=avg_loss, acc=f\"{accuracy:.2f}%\")\n\n    avg_loss = running_loss / total if total > 0 else 0\n    accuracy = 100.0 * correct / total if total > 0 else 0\n\n    print(f\"\\nâœ… Validation Results:\\n\"\n          f\"ğŸ”¹ Average Loss: {avg_loss:.4f}\\n\"\n          f\"ğŸ”¹ Accuracy: {accuracy:.2f}%\\n\")\n\n    return avg_loss, accuracy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:56:34.617005Z","iopub.execute_input":"2025-05-30T09:56:34.617335Z","iopub.status.idle":"2025-05-30T09:56:34.634722Z","shell.execute_reply.started":"2025-05-30T09:56:34.617295Z","shell.execute_reply":"2025-05-30T09:56:34.633815Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(f\"ğŸ“¦ Number of batches available in training: {len(train_loader)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:56:34.635762Z","iopub.execute_input":"2025-05-30T09:56:34.636041Z","iopub.status.idle":"2025-05-30T09:56:34.659371Z","shell.execute_reply.started":"2025-05-30T09:56:34.636017Z","shell.execute_reply":"2025-05-30T09:56:34.658578Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 7.5 Execute Final Training","metadata":{}},{"cell_type":"code","source":"import torch\nimport os\n\nbase_save_path = \"/kaggle/working/\"\nbest_model_path = os.path.join(base_save_path, \"best_model.pth\")\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nstart_epoch = 1\nnum_epochs = 10  # ğŸ”§ Ø¹Ø¯Ø¯ Epochs Ø§Ù„ÙƒÙ„ÙŠ Ø§Ù„Ù„ÙŠ ØªØ¨ÙŠ ØªØ¯Ø±Ø¨ Ø¹Ù„ÙŠÙ‡\n\n# âœ… ØªØ­Ù…ÙŠÙ„ Ø£ÙØ¶Ù„ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­ÙÙˆØ¸ Ø¥Ù† ÙˆØ¬Ø¯\nif os.path.exists(best_model_path):\n    print(\"âœ… Loading best saved model...\")\n    model.load_state_dict(torch.load(best_model_path, map_location=device))\n    model.to(device)\n\n    training_log = os.path.join(base_save_path, \"training_results.txt\")\n    if os.path.exists(training_log):\n        with open(training_log, \"r\", encoding=\"utf-8\") as f:\n            lines = f.readlines()\n            for line in reversed(lines):\n                if \"Epoch\" in line and \"/\" in line:\n                    start_epoch = int(line.split(\" \")[1].split(\"/\")[0]) + 1\n                    break\n\n    print(f\"ğŸš€ Starting training from Epoch {start_epoch} using the best saved model.\")\n\n# âœ… Ø¨Ø¯Ø¡ Ù…Ù† Ø§Ù„ØµÙØ± Ø¥Ø°Ø§ Ù„Ù… ÙŠÙˆØ¬Ø¯ Ø£ÙŠ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø­ÙÙˆØ¸\nelse:\n    print(\"âŒ No saved model found! Starting training from scratch.\")\n    model.to(device)\n\n# âœ… Ø¥Ø¹Ø¯Ø§Ø¯ Optimizer\noptimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-3)\n\n# âœ… Ø¥Ø¹Ø¯Ø§Ø¯ Scheduler Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… OneCycleLR\nscheduler = torch.optim.lr_scheduler.OneCycleLR(\n    optimizer,\n    max_lr=0.003,\n    steps_per_epoch=len(train_loader),\n    epochs=num_epochs - start_epoch + 1,  # â³ Ø­Ø³Ø¨ Ù…Ø§ ØªØ¨Ù‚Ù‰ Ù…Ù† Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n    pct_start=0.2,\n    anneal_strategy='cos',\n    div_factor=25,\n    final_div_factor=1000\n)\n\ntorch.cuda.empty_cache()\n\n# âœ… Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨\ntrain_model(\n    model,\n    train_loader,\n    validation_loader,\n    criterion,\n    optimizer,\n    scheduler,\n    num_epochs,\n    base_save_path,\n    start_epoch\n)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:56:34.660196Z","iopub.execute_input":"2025-05-30T09:56:34.660418Z","iopub.status.idle":"2025-05-30T10:16:41.380116Z","shell.execute_reply.started":"2025-05-30T09:56:34.660401Z","shell.execute_reply":"2025-05-30T10:16:41.379272Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 9. Final Testing","metadata":{}},{"cell_type":"markdown","source":"### 9.1 Evaluate on Test Set","metadata":{}},{"cell_type":"code","source":"\nimport os\nimport shutil\nimport numpy as np\nimport torch\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nfrom collections import Counter\nfrom tqdm import tqdm\nfrom torchvision import datasets, transforms, models\nfrom torch.utils.data import DataLoader\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:16:41.381294Z","iopub.execute_input":"2025-05-30T10:16:41.381549Z","iopub.status.idle":"2025-05-30T10:16:42.990831Z","shell.execute_reply.started":"2025-05-30T10:16:41.381518Z","shell.execute_reply":"2025-05-30T10:16:42.990227Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from collections import Counter\n\nlabel_counts = Counter([label for _, label in test_dataset.samples])\nprint(\"ğŸ“Š Test dataset image distribution:\")\nfor class_name, count in zip(test_dataset.classes, label_counts.values()):\n    print(f\"ğŸ“ {class_name}: {count} images\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:16:42.991623Z","iopub.execute_input":"2025-05-30T10:16:42.991991Z","iopub.status.idle":"2025-05-30T10:16:42.998840Z","shell.execute_reply.started":"2025-05-30T10:16:42.991973Z","shell.execute_reply":"2025-05-30T10:16:42.998218Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def show_images(dataset, num_images=6):\n    fig, axes = plt.subplots(1, num_images, figsize=(12, 4))\n    for i in range(num_images):\n        img, label = dataset[i]\n        img = img.permute(1, 2, 0).numpy() * 0.5 + 0.5  # ÙÙƒ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ·Ø¨ÙŠØ¹\n        axes[i].imshow(img)\n        axes[i].set_title(dataset.classes[label])\n        axes[i].axis(\"off\")\n    plt.tight_layout()\n    plt.show()\n\nshow_images(test_dataset)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:16:42.999749Z","iopub.execute_input":"2025-05-30T10:16:43.000034Z","iopub.status.idle":"2025-05-30T10:16:43.954272Z","shell.execute_reply.started":"2025-05-30T10:16:42.999999Z","shell.execute_reply":"2025-05-30T10:16:43.953427Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom datetime import datetime\n\ndef test_model(model, test_loader, criterion, device):\n    model.eval()\n    running_loss, correct, total = 0.0, 0, 0\n    with torch.no_grad():\n        for images, labels in test_loader:\n            images, labels = images.to(device), labels.to(device)\n            outputs = model(images)\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item() * images.size(0)\n            correct += outputs.argmax(dim=1).eq(labels).sum().item()\n            total += labels.size(0)\n\n    avg_loss = running_loss / total if total > 0 else 0\n    accuracy = 100.0 * correct / total if total > 0 else 0\n    print(f\"ğŸ¯ Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬: Loss = {avg_loss:.4f}, Ø¯Ù‚Ø© = {accuracy:.2f}%\")\n\n    return avg_loss, accuracy\n\n# ØªÙ†ÙÙŠØ° Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\ntest_loss, test_acc = test_model(model, test_loader, criterion, device)\n\n# ØªØ­Ø¯ÙŠØ¯ Ù…Ø³Ø§Ø± Ø­ÙØ¸ Ø§Ù„Ù†ØªØ§Ø¦Ø¬\naccuracy_path = \"/kaggle/working/accuracy0.txt\"\nos.makedirs(os.path.dirname(accuracy_path), exist_ok=True)  # Ø§Ù„ØªØ£ÙƒØ¯ Ù…Ù† ÙˆØ¬ÙˆØ¯ Ø§Ù„Ù…Ø¬Ù„Ø¯\n\n# ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ù†ØªÙŠØ¬Ø© Ù…Ø¹ Ø§Ù„ØªØ§Ø±ÙŠØ® ÙˆØ§Ù„ÙˆÙ‚Øª\ntimestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\nwith open(accuracy_path, \"a\", encoding=\"utf-8\") as f:\n    f.write(f\"[{timestamp}] Accuracy: {test_acc:.2f}%, Loss: {test_loss:.4f}\\n\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:16:43.955265Z","iopub.execute_input":"2025-05-30T10:16:43.955483Z","iopub.status.idle":"2025-05-30T10:17:11.371007Z","shell.execute_reply.started":"2025-05-30T10:16:43.955466Z","shell.execute_reply":"2025-05-30T10:17:11.369981Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"\n### 9.3 Classification Report","metadata":{}},{"cell_type":"code","source":"import os\nimport torch\nfrom sklearn.metrics import classification_report\nfrom datetime import datetime\n\ntrue_labels = []\npredicted_labels = []\n\nwith torch.no_grad():\n    for images, labels in test_loader:\n        images, labels = images.to(device), labels.to(device)\n        outputs = model(images)\n        predictions = outputs.argmax(dim=1).cpu().numpy()\n\n        true_labels.extend(labels.cpu().numpy())\n        predicted_labels.extend(predictions)\n\nreport = classification_report(true_labels, predicted_labels, target_names=[\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\"])\nprint(\"ğŸ“„ Classification Report:\\n\", report)\n\nreport_path = \"/kaggle/working/classification_report.txt\"\nwith open(report_path, \"w\", encoding=\"utf-8\") as f:\n    f.write(f\"ğŸ“Œ Classification Report - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\n    f.write(\"=\" * 50 + \"\\n\")\n    f.write(report)\n    f.write(\"\\n\" + \"=\" * 50 + \"\\n\")\n\nprint(f\"âœ… Classification report saved at: {report_path}\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:17:11.372546Z","iopub.execute_input":"2025-05-30T10:17:11.372900Z","iopub.status.idle":"2025-05-30T10:17:39.126212Z","shell.execute_reply.started":"2025-05-30T10:17:11.372851Z","shell.execute_reply":"2025-05-30T10:17:39.125429Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 9.4  confusion_matrix","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\n\nconf_matrix = confusion_matrix(true_labels, predicted_labels)\n\nxlabel = \"Predictions\"\nylabel = \"True Values\"\ntitle = \"Confusion Matrix\"\n\nplt.figure(figsize=(7, 6))\nsns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=True,\n            xticklabels=[\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\"],\n            yticklabels=[\"Class 0\", \"Class 1\", \"Class 2\", \"Class 3\"])\n\nplt.xlabel(xlabel, fontsize=14, labelpad=10, fontweight=\"bold\")\nplt.ylabel(ylabel, fontsize=14, labelpad=10, fontweight=\"bold\")\nplt.xticks(rotation=0, fontsize=12)\nplt.yticks(rotation=0, fontsize=12)\nplt.title(title, fontsize=16, pad=15, fontweight=\"bold\")\n\nconf_matrix_path = \"/kaggle/working/confusion_matrix.png\"\nplt.savefig(conf_matrix_path, dpi=300, bbox_inches=\"tight\")\n\n#print(f\"âœ… The confusion matrix has been saved at: {conf_matrix_path}\")\nplt.show()  \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:17:39.127225Z","iopub.execute_input":"2025-05-30T10:17:39.127455Z","iopub.status.idle":"2025-05-30T10:17:39.705232Z","shell.execute_reply.started":"2025-05-30T10:17:39.127437Z","shell.execute_reply":"2025-05-30T10:17:39.704488Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}